{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18f8bb38-0420-41a3-823a-66ea8a180515",
   "metadata": {},
   "source": [
    "## 1. After each stride-2 conv, why do we double the number of filters?\n",
    "**Answer:**\n",
    "After each stride-2 convolution, we typically double the number of filters to compensate for the reduction in the spatial dimensions (height and width) of the feature maps. As the spatial size decreases, the network is able to focus on more abstract and complex features. By increasing the number of filters, we ensure that the model can learn more features, maintaining the network's capacity to capture detailed information at deeper layers.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Why do we use a larger kernel with MNIST (with simple cnn) in the first conv?\n",
    "**Answer:**\n",
    "A larger kernel in the first convolutional layer is often used with datasets like MNIST to capture broader and more meaningful features from the input image. Since MNIST images are relatively simple and small (28x28 pixels), using a larger kernel (e.g., 5x5) helps in detecting larger patterns, such as edges and shapes, right from the initial layer, which is crucial for accurate digit recognition.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. What data is saved by ActivationStats for each layer?\n",
    "**Answer:**\n",
    "`ActivationStats` typically saves the following data for each layer:\n",
    "- **Mean:** The average activation value for the neurons in the layer.\n",
    "- **Standard deviation:** The spread of the activation values around the mean.\n",
    "- **Histograms:** Distribution of activation values to understand the layer's behavior.\n",
    "- **Activation values:** Sometimes the actual activation values for each neuron can be stored for further analysis or debugging.\n",
    "\n",
    "These statistics are useful for diagnosing issues like vanishing/exploding gradients and ensuring that the activations are within a reasonable range during training.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. How do we get a learner's callback after they’ve completed training?\n",
    "**Answer:**\n",
    "To get a learner's callback after they’ve completed training, you can use the `learn.callbacks` attribute in most deep learning frameworks like PyTorch or Fastai. After training is completed, you can access specific callback information as follows:\n",
    "```python\n",
    "callbacks = learn.callbacks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd9acf6-e5dd-4114-a8df-ddb8622be46b",
   "metadata": {},
   "source": [
    "## 5. What are the drawbacks of activations above zero?\n",
    "**Answer:**\n",
    "Activations above zero can have several drawbacks:\n",
    "- **Exploding gradients:** If activations are consistently above zero, the gradients during backpropagation can grow too large, leading to unstable training and potentially causing the model to diverge.\n",
    "- **Overfitting:** Large activations might cause the model to overfit the training data because the model may become overly sensitive to the specific patterns in the training set rather than learning generalizable features.\n",
    "- **Saturation:** In activation functions like sigmoid or tanh, large positive activations can push the function into its saturation region, where the gradient is very small. This slows down learning as updates to the weights become minimal.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Draw up the benefits and drawbacks of practicing in larger batches?\n",
    "**Answer:**\n",
    "**Benefits:**\n",
    "- **Stable Gradient Estimates:** Larger batches tend to provide more accurate and stable estimates of the gradient, which can lead to smoother and potentially faster convergence during training.\n",
    "- **Efficient Computation:** Larger batch sizes can make better use of parallel processing capabilities of modern GPUs, leading to faster computation and shorter training times.\n",
    "\n",
    "**Drawbacks:**\n",
    "- **Increased Memory Usage:** Larger batches require more memory, which may limit the maximum batch size, especially on GPUs with limited VRAM.\n",
    "- **Less Regularization:** Smaller batches introduce more noise into the gradient estimation process, which can act as a form of regularization and help prevent overfitting. Larger batches reduce this noise, which can make the model more prone to overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Why should we avoid starting training with a high learning rate?\n",
    "**Answer:**\n",
    "Starting training with a high learning rate can cause several issues:\n",
    "- **Divergence:** A high learning rate may cause the model to take too large steps during gradient descent, potentially overshooting the optimal solution, leading to divergence where the loss increases rather than decreases.\n",
    "- **Instability:** High learning rates can make the training process unstable, with the loss fluctuating wildly or failing to converge.\n",
    "- **Poor Local Minima:** Even if the model converges, it may do so to a suboptimal solution, missing out on finding the best possible parameters due to the large steps skipping over finer adjustments.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. What are the pros of studying with a high rate of learning?\n",
    "**Answer:**\n",
    "- **Faster Convergence:** A high learning rate can speed up the convergence of the model by allowing it to make significant progress towards the optimal solution in the initial stages of training.\n",
    "- **Escaping Local Minima:** A higher learning rate can help the model jump out of local minima, potentially finding a better, more generalizable solution.\n",
    "- **Efficient Early Training:** In the early stages of training, when the model is far from the optimum, a high learning rate can quickly reduce the loss, bringing the model closer to a good solution more efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Why do we want to end the training with a low learning rate?\n",
    "**Answer:**\n",
    "Ending the training with a low learning rate helps in fine-tuning the model:\n",
    "- **Precise Adjustment:** A low learning rate allows for smaller, more precise adjustments to the model parameters, helping the model converge to a better and more refined solution.\n",
    "- **Avoid Overshooting:** It reduces the risk of overshooting the minimum of the loss function, leading to more stable and reliable convergence.\n",
    "- **Final Tuning:** As the model nears the optimal solution, a low learning rate helps in careful tuning, ensuring that the model settles into the best possible state with minimal error.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214ac2da-39ea-4987-9abc-7ae85274fceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
